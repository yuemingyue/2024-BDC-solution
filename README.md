
比赛链接：[https://www.heywhale.com/org/bdc/competition](https://www.heywhale.com/org/bdc/competition/area/6662bf9a8d6c97c5d0c6bb10/content)
# 代码说明
数据分布不一致就不说了，第一次碰上复现还能shake的，懒得喷....
## 环境配置（必选）
使用的是官方镜像：Torch 2.2.2 Cuda 12.1 Python 3.11.8 Ubuntu 22.04

## 算法（必选）

我们主要的思路是模型融合+一些小trick

算法的主要是使用了itransformer模型还有softs模型

我们采用了Warm up和混合loss进行训练，让模型尽可能收敛到最优状态，并且我们的两个模型差异度足够因此我们得到很大的收益

### 整体思路介绍（必选）

1.模型训练：

采用了 warm和混合loss进行训练

2.模型结构：

我们在softs的结构中加入了DAIN,进行自适应，让模型在测试集上可以得到更好的适应效果。

3.模型融合：

我们模型融合的依据来自于单模的线上得分，itrans 单模分数为1.198 softs单模分数为1.215

4.多尺度融合

我们发现初赛的模型在24的预测上远远好于复赛的前24小时预测，因此我们发现两个模型在短期预测上是远远好于长期预测，因此我们以短期预测规范长期预测，训练了预测24小时，以及预测48小时的模型与长期预测的模型进行融合，得到了很好效果

5.TTA

我们的TTA主要采用了多插值融合的办法，即我们的模型在一种插值上进行训练，但是在多种插值上进行预测，这种方法能够适应多种不同的预测数据，在测试集上得到了很好的提升。

### 方法的创新点（如果有）

1.DAIN层

2.混合Loss（即FFTLoss,将值域预测转到频域预测能提高长期预测的效果）

3.短期预测规范长期预测

4.多插值的TTA策略

### 损失函数

我们使用了多种损失函数，比如 FFTLoss,MSELoss,SmoothL1Loss

并且我们在训练时是采用了其中两两混合的loss进行训练，保证了模型的效果。

### 数据预处理

我们通过数据挖掘发现，数据中有许多的脏数据，主要表现为温度的不一致性，我们通过比较协变量和因变量的差距，将差距过大的站点认为是脏数据站点进行剔除。

### 模型集成

1.单模自融合

我们将单个模型训练过程中的多个epo结合起来，可以抑制单个模型的过拟合。

2.多异质模型融合：

我们模型融合的依据来自于单模的线上得分，itrans 单模分数为1.198 softs单模分数为1.215

3.多尺度融合

我们发现初赛的模型在24的预测上远远好于复赛的前24小时预测，因此我们发现两个模型在短期预测上是远远好于长期预测，因此我们以短期预测规范长期预测，训练了预测24小时，以及预测48小时的模型与长期预测的模型进行融合，得到了很好效果

4.TTA

我们的TTA主要采用了多插值融合的办法，即我们的模型在一种插值上进行训练，但是在多种插值上进行预测，这种方法能够适应多种不同的预测数据，在测试集上得到了很好的提升。

### 算法的其他细节

## 训练和测试流程（必选）
运行train.ipynb

## 其他注意事项

无
